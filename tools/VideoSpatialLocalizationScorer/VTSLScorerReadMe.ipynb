{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Spatial-Temporal Localization Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0\\. [Synopsis](#Synopsis)\n",
    "\n",
    "1\\. [Description](#Description)\n",
    "\n",
    "2\\. [Spatial-Temporal System Masks](#Sysmasks)\n",
    "\n",
    "3\\. [Reference Masks](#Refmasks)\n",
    "\n",
    "3.1\\. [Binarizing the Reference Mask](#Binmasks)\n",
    "\n",
    "4\\. [Generating No-Score Zones](#noscorezone)\n",
    "\n",
    "5\\. [Metrics](#metrics)\n",
    "\n",
    "5.1 [System Mask Thresholding](#metricsthresholding)\n",
    "\n",
    "5.2 [Notation](#metricsnotation)\n",
    "\n",
    "5.3 [Matthews Correlation Coefficient (MCC)](#metricsmcc)\n",
    "\n",
    "5.4 [Nimble Mask Metric (NMM)](#metricsnmm)\n",
    "\n",
    "5.5 [Weighted L1 Loss (BWL1 and GWL1)](#metricswl1)\n",
    "\n",
    "5.6 [Area Under Curve (AUC)](#metricsauc)\n",
    "\n",
    "5.7 [Equal Error Rate (EER)](#metricseer)\n",
    "\n",
    "6\\. [ROC Curves](#roccurves)\n",
    "\n",
    "7\\. [Options](#options)\n",
    "\n",
    "7.1 [Task Type Options](#optionstask)\n",
    "\n",
    "7.2 [Input Options](#optionsinputs)\n",
    "\n",
    "7.3 [Output Options](#optionsoutputs)\n",
    "\n",
    "7.4 [Printout Options](#optionsprintout)\n",
    "\n",
    "7.5 [Scoring Options](#optionsscoring)\n",
    "\n",
    "7.6 [Performance Evaluation by Query](#optionsquery)\n",
    "\n",
    "8\\. [Examples](#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Synopsis<a name=\"Synopsis\"></a>\n",
    "\n",
    "python2 VideoSpatialLocalizationScorer.py<br/>\n",
    "                      --refDir reference-directory<br/>\n",
    "                      -r reference-file<br/>\n",
    "                      -x index-file<br/>\n",
    "                      -s system-output-file<br/>\n",
    "                      --outRoot output-directory<br/>\n",
    "                      [options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description<a name=\"Description\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The video spatial-temporal localization (VSTL) scorer calculates performance scores that measure the accuracy of the system output masks to their corresponding reference masks. This scorer's function is analogous to that of the image localization scorer (under tools/MaskScorer), but is applied to detecting manipulated pixels in videos.\n",
    "\n",
    "The script produces output files in the form of pipe-separated (\"|\") CSV files, one containing scores at the trial level (\\*-pervideo.csv), another containing an average of the metrics in the first CSV (\\*-mask_score.csv), and a third containing a list of manipulations that were evaluated or skipped over (\\*-journalResults.csv).\n",
    "\n",
    "The mask scorer takes the following steps to score a mask:\n",
    "\n",
    "1. Reads in the reference and system masks.\n",
    "2. Binarizes the reference mask.\n",
    "3. Generates the no-score zone.\n",
    "4. Computes metrics \n",
    "\n",
    "If the script aborts due to an error, the error will be written to standard out, and the script will exit with a status of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spatial-Temporal System Masks<a name=\"Sysmasks\"></a>\n",
    "\n",
    "The system mask is read in as an HDF5 file. Each file contains a set of blocks indexed according to their first frames (with the convention that frame 1 is the starting frame). Each block, representing a contiguous set of manipulated frames, is a three-dimensional integer matrix with entries ranging from 0 to 255, the values of which denote whether or not a particular pixel is manipulated (0 meaning it is, 255 meaning it is not). The dimensions of each block is according to the following convention: (total_frames,height,width).\n",
    "\n",
    "Thus each block in the HDF5 system mask denotes a range of frames where the performer's algorithm detects localizable manipulations. The validator will fail the mask if it does not conform to the desired specifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reference Masks<a name=\"Refmasks\"></a>\n",
    "\n",
    "The reference mask is read in as an HDF5 mask containing blocks indexed according to the same convention as the system output mask. A fourth dimension might be added that adds layers with additional manipulations for the same block, leading to the following indexing scheme for each block in the reference mask: (total_frames,height,width,layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Binarizing the Reference Mask<a name=\"Binmasks\"></a>\n",
    "\n",
    "In order to score the system output mask against the reference mask, the reference mask needs to be converted to a single-channel mask. The video spatial localization scorer handles this by selecting the set of Bit Planes that correspond to manipulations for the probe and using this as the ground truth positives.\n",
    "\n",
    "If selective scoring is utilized (via the -qm option), only regions that match the query will be scored against. Regions listed in the journal as explicitly non-matching will be treated as no-score regions, which will be discussed below. Other regions that are not listed in the journal are ignored, whether due to later manipulations recorded in the same journals that do not apply to the probe in question or due to otherwise faulty data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating No-Score Zones<a name=\"noscorezone\"></a>\n",
    "\n",
    "The following information is identical to that provided in the image localization scorer under tools/MaskScorer.\n",
    "\n",
    "No-score zones are automatically generated to grant an amount of flexibility to the performer, to exclude manipulated regions that are not of interest, and to exclude regions that the performer does not deem of interest. Each subset of the no-score zone is denoted by a specified type of no-score zone based on these motivations: the boundary no-score zone, selective no-score zone, and pixel no-score zone respectively.\n",
    "\n",
    "To ease the demand in identifying the region manipulated, the boundary around each region is eroded and dilated, with the difference serving as the boundary no-score zone.\n",
    "\n",
    "If selective scoring is utilized (via the -qm option), the mask scorer also generates the distraction no-score zone by dilating the positive pixels of the irrelevant regions in the reference mask. This number need not be the same as the dilation parameter used to generate the boundary no-score zone and may be adjusted via option --ntdks.\n",
    "\n",
    "An additional system-generated no-score zone may be used by the performer. The performer specifies a particular pixel value to treat as a no-score zone via option --nspx, and all pixels with the same value in the system mask serves as the no-score zone.\n",
    "\n",
    "The three are merged to generate the overall no-score zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics<a name=\"metrics\"></a>\n",
    "\n",
    "The system computes the following metrics for Spatial-Temporal scoring. Only the MCC is computed for Temporal scoring; it is labeled \"TemporalMCC\":\n",
    " * Matthews Correlation Coefficient (MCC)\n",
    " * Nimble Mask Metric (NMM)\n",
    " * Weighted L1 Loss: Binary (BWL1) and Grayscale (GWL1)\n",
    " * Area Under the ROC Curve (AUC)\n",
    " * Equal Error Rate (EER)\n",
    "\n",
    "The details below concerning the metrics are identical to those of the image localization scorer, and are duplicated here for ease of reference.\n",
    "\n",
    "### 5.1 System Mask Thresholding<a name=\"metricsthresholding\"></a>\n",
    "\n",
    "Most of these metrics depend on the binarized values of the grayscale system mask. There are various ways binarize the system mask, namely by picking the threshold that determines whether to treat a particular grayscale pixel as white or black.\n",
    "\n",
    "One is to threshold each mask according to whether a threshold returns the greatest MCC; these are called the \"optimum\" metrics. Another is to specify a fixed threshold for all masks to be binarized with the --sbin option; the metrics computed from this fixed threshold are called the \"actual\" metrics. Specifying said threshold will also prompt the scorer to compute the threshold that yields the maximum average MCC; the metrics computed by this threshold are called the \"maximum\" metrics.\n",
    "\n",
    "Averages of the metrics will be recorded across all probes. The average and standard deviation of the optimum thresholds will also be recorded.\n",
    "\n",
    "### 5.2 Notation<a name=\"metricsnotation\"></a>\n",
    "\n",
    "The following notation is used in discussing the metrics:\n",
    " * $gt$ refers to the binarized reference mask discussed in section 3.\n",
    " * $sys_{\\theta}$ refers to the system output mask binarized by specified threshold $\\theta$\n",
    "   * $sys_{*}$ refers to the unbinarized grayscale system output mask\n",
    " * $wts$ is a matrix of the overall no-score region discussed in section 4.\n",
    "   * The no-score region can be further subdivided into the boundary, distraction, and system-generated no-score zones. The pixel counts corresponding to these regions are denoted $BNS$, $SNS$, and $PNS$ respectively. In cases where regions from different types of no-score zone intersect, the pixels count towards $PNS$, then $SNS$, or $BNS$, in order of decreasing priority.\n",
    " * $TP$, $TN$, $FP$, and $FN$ are functions of $gt$, $sys_{\\theta}$, and $wts$. All four are measures derived from the confusion matrix. Pixels for which the weights matrix is equal to 0 are not counted in the confusion measures:\n",
    "   * $TP$ refers to the total number of True Positives, pixels where $gt$ is positive and $sys_{\\theta}$ is thresholded positive (manipulated)\n",
    "   * $TN$ refers to the total number of True Negatives, pixels where $gt$ is negative and $sys_{\\theta}$ is positive (not manipulated)\n",
    "   * $FP$ refers to the total number of False Positives, pixels where $gt$ is negative but $sys_{\\theta}$ is positive.\n",
    "   * $FN$ refers to the total number of False Negatives, pixels where $gt$ is positive but $sys_{\\theta}$ is negative.\n",
    "\n",
    "### 5.3 Matthews Correlation Coefficient (MCC)<a name=\"metricsmcc\"></a>\n",
    "\\begin{equation*}\n",
    "MCC(gt,sys_{\\theta}) = \\frac{TP*TN - FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{equation*}\n",
    "\n",
    "An MCC of 1 denotes perfect correlation, an MCC of 0 denotes no correlation at all, and an MCC of -1 denotes perfect anti-correlation. If any of the sums in the denominator equals 0, the MCC is taken to be 0 by convention.\n",
    "\n",
    "### 5.4 Nimble Mask Metric (NMM)<a name=\"metricsnmm\"></a>\n",
    "\\begin{equation*}\n",
    "NMM(gt,sys_{\\theta},wts,c)=\\max{\\left(\\frac{TP - FN - FP}{\\sum\\limits_{px\\in gt}wts(px)},c\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\Sigma_{px \\in GT} wts(px)$ refers to the sum over the pixels in the ground truth that are marked black. $c$ denotes a minimum cutoff value for the scoring to have any meaning; by default, $c=-1$.\n",
    "\n",
    "### 5.5 Weighted L1 Loss (BWL1 and GWL1)<a name=\"metricswl1\"></a>\n",
    "\n",
    "A Weighted L1 of 0 denotes perfect or near perfect match up to variation within the weights that are 0; 1 denotes perfect mismatch. $(FP+FN)_{weights > 0}$ refers to the total number of $FP$ and $FN$ pixels where weights are greater than 0.\n",
    "\n",
    "The Weighted L1 is applied separately to the original grayscale system output and the binarized mask, producing the grayscale Weighted L1 (GWL1) and binarized Weighted L1 (BWL1) metrics respectively. In the case of the original grayscale, the value is summed over the weighted difference in pixel intensity.\n",
    "\n",
    "\\begin{equation*}\n",
    "BWL1(gt,sys_{\\theta},wts)=\\frac{(FP+FN)_{wts > 0}}{\\sum\\limits_{px\\in gt}wts(px)}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "GWL1(gt,sys_{*},wts)=\\frac{\\sum\\limits_{px\\in gt} \\left|gt(px) - sys_{*}(px)\\right|wts(px)}{\\sum\\limits_{px\\in gt}wts(px)}\n",
    "\\end{equation*}\n",
    "\n",
    "### 5.6 Area Under Curve (AUC)<a name=\"metricsauc\"></a>\n",
    "\n",
    "The possibility of multiple thresholds giving rise to a set of varying confusion measures gives rise to the receiver operating characteristic (ROC). The ROC curve measures a relation between the true positive rate ($TPR$) and false positive rate ($FPR$), defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "\\end{equation*}\n",
    "\n",
    "The area under the ROC curve (AUC) is a measure for the accuracy of the system.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "AUC(gt,sys_{*},wts) = \\frac{1}{2}\\sum\\limits_{\\theta_{n} \\in \\{\\theta_{0},\\ldots,\\theta_{N-1}\\}} \\left(TPR(gt,sys_{\\theta_{n+1}},wts) + TPR(gt,sys_{\\theta_{n}},wts)\\right)\\left(FPR(gt,sys_{\\theta_{n+1}},wts) - FPR(gt,sys_{\\theta_{n}},wts)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "### 5.7 Equal Error Rate (EER)<a name=\"metricseer\"></a>\n",
    "The value of the $FPR$ at the threshold where it is equal to the $FNR$ (or vice versa). $FPR$ is given by the formula in 5.6. $FNR$ is simply $1 - TPR$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC Curves<a name=\"roccurves\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pixel-weighted average ROC curve and probe-weighted average ROC curve will be recorded and defined across all thresholds relevant to all system masks. Computing the pixel- and probe-weighted average ROC curves is exactly the same as in the image localization scorer.\n",
    "\n",
    "For a given threshold $\\theta$, the true positive rate (TPR) for the ROC curves are as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR_{pixel}(\\theta) = \\frac{\\sum_{probes}TP_{\\theta}}{\\sum_{probes}\\left(TP_{\\theta} + FN_{\\theta}\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR_{probe}(\\theta) = \\frac{1}{|probes|}\\sum_{probes}\\frac{TP_{\\theta}}{TP_{\\theta} + FN_{\\theta}}\n",
    "\\end{equation*}\n",
    "\n",
    "The false postive rates (FPR) are defined similarly. The pixel weighted average and probe weighted average ROC curves are drawn from the pairs of (FPR,TPR) points. Their AUC's are recorded with the average metrics mentioned at the end of section 5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Options<a name=\"options\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command-line options for the mask scorer can be categorized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Task Type Options<a name=\"optionstask\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-t --task [manipulation]\n",
    "\n",
    "  * Specify the task type for evaluation (default = manipulation). As the manipulation task is the only one defined for video spatial localization at the moment, this option need not be selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Input Options<a name=\"optionsinputs\"></a>\n",
    "\n",
    "All CSV files passed to the Mask Scorer must contain headers and must have their columns separated by pipe characters ('|'). Fields and values in the CSV should <i>not</i> be enclosed in quotes ( ' or \" ) if possible (e.g. entries 'foo', an empty field, and 'bar', in that order, should look like this in the csv: foo||bar). Additional specifications for the index and system output files can be found in the ValidatorNotebook.html file under the Validator directory.\n",
    "\n",
    "All of the options here are identical in function to their image localization scorer counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--refDir\n",
    "\n",
    "  * Specify the path to the dataset (e.g. \"/NC2016_Test0601\"). Note that this path must be specified as masks will be read relative to this directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-r --inRef\n",
    "\n",
    "  * Specify the reference CSV file within refDir that contains the ground-truth information and metadata about each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-x --inIndex\n",
    "\n",
    "  * Define the index CSV file within refDir. The index file contains the TaskID, ProbeFileID, ProbeFileName, ProbeWidth, and ProbeHeight fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--sysDir\n",
    "\n",
    "  * Specify the system output data path, for example \"mysysoutput/\" (default = .) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-s --inSys\n",
    "\n",
    "  * Specify the CSV file of the system performance results formatted according to NC2016 specification. The file must contain the ProbeFileID, ConfidenceScore, and OutputProbeMaskFileName fields, in that order, and if scoring on the splice task, the ProbeFileID, DonorFileID, ConfidenceScore, OutputProbeMaskFileName, and OutputDonorMaskFileName fields, in that order. The OutputProbeMaskFileNames and OutputDonorMaskFileNames (where relevant) should be directory strings relative to the location of the system performance CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--sbin\n",
    "\n",
    "  * Evaluate the system output masks as binarized masks with a numeric threshold in the interval [-1,255]. -1 is allowed to give the performer the option to binarize the entire mask to white. Choosing -10 will forego this option. (default = -10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Output Options<a name=\"optionsoutputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--outRoot\n",
    "\n",
    "  * Specify the report output path and the file prefix joined by a '/'. For example, specifying \"--outRoot test/output\" for a submission NIST_001 will output the aggregate score report \"output_mask_score.csv\" and the per-image report \"output_mask_scores_perimage.csv\" in the \"test\" directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Printout Options<a name=\"optionsprintout\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-v verbose\n",
    "\n",
    "  * Control print output. Select 1 to print all non-error related output and 0 to suppress all print output (bar argument-parsing errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Scoring Options<a name=\"optionsscoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--eks\n",
    "\n",
    "  * Erosion kernel size. (number must be odd; default = 15)\n",
    "  \n",
    "--dks\n",
    "\n",
    "  * Dilation kernel size. (number must be odd; default = 11)\n",
    "\n",
    "--ntdks\n",
    "\n",
    "  * Non-target dilation kernel for regions that the user does not want scored. (number must be odd; default = 15)\n",
    "\n",
    "--nspx\n",
    "\n",
    "  * Set a pixel value in the system output mask to be a no-score region. The pixel value must be in the interval [0,255]. -1 indicates that no particular pixel value will be chosen to be the no-score zone. (default = -1)\n",
    "\n",
    "--pppns perProbePixelNoScore\n",
    "\n",
    "  * \"Use the pixel values in the OptOutPixel column of the system output to designate no-score zones. This value will override the value set for the global no-score pixel.\n",
    "\n",
    "-k kernel\n",
    "\n",
    "  * The shape of the kernel to be used, for both erosion and dilation. Choose from 'box','disc','diamond','gaussian', or 'line'. The default kernel is 'box'.\n",
    "  \n",
    "--optOut\n",
    "\n",
    "  * Evaluate algorithm performance on a select number of trials determined by the performer via values in the ProbeStatus column. The values in the column that are opted out of the evaluation are \"NonProcessed\" (denoting some kind of system failure on that probe), \"OptOutLocalization\" (denoting opting out of the localization task only), and \"OptOutAll\" (denoting opting out of both the detection and localization tasks).\n",
    "  \n",
    "--precision\n",
    "\n",
    "  * The number of digits to round computed scores. Note that rounding is not absolute, but is by significant digits (e.g. a score of 0.003333333333333... will round to 0.0033333 for a precision of 5). (default = 16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5.1 Temporal Scoring Options\n",
    "\n",
    "The following options were taken from or inspired by the Video Temporal Localization Scorer and apply primarily to temporal scoring, but have analogous effects in Spatial-Temporal scoring when set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-c, --collars\n",
    "\n",
    "  * An integer designating the number of frames to add to each side of the endpoints of the reference intervals, generating a temporal boundary no-score zone. In spatial-temporal localization scoring, every pixel of a collared frame is treated as a boundary no-score zone.\n",
    "\n",
    "--truncate\n",
    "\n",
    "  * Truncates any system intervals that go beyond the video reference framecount, setting them to the framecount value and enabling them to be scored.\n",
    "  \n",
    "--temporal_gt_only\n",
    "\n",
    "  * Scores only on frames where there is temporal localization manipulation.\n",
    "\n",
    "--temporal_scoring_only\n",
    "\n",
    "  * Generate only the temporal localization metrics, skipping the spatial localization metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Performance Evaluation by Query<a name=\"optionsquery\"></a>\n",
    "\n",
    "This option allows the user to evaluate their algorithm performance on either subsets or partitions of the data based on the specified queries and query options. The reference and index CSV files contain a list of factors (e.g., ProbePostProcessed|DonorPostProcessed|ManipulationQuality|IsManipulationTypeRemoval|...). Selecting none of the following factors will output a single report table (CSV) over the entire computed dataset.\n",
    "\n",
    "All of the options here are identical in function to their image localization scorer counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-q query\n",
    " * Evaluate algorithm performance on a partitioned dataset using multiple factor queries, one at a time (e.g. \"Collection==['NC2017'] & Purpose==['add','remove']\" will average over the rows that fit this criterion for one queried average, but \"Collection==['NC2017']\" \"Purpose==['add','remove']\" will average over the first and then the second independently for two queried averages). The option generates N report tables (CSV), one for each query.\n",
    "   * Syntax: -q \"query1\" \"query2\" \"query3\" ...\n",
    "   - The syntax is the same as Pandas' query syntax. Please see the detailed query rule in the website: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-query.\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI']\" => 1 query\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI'] and PostProcessing=['rescale']\" => 1 query\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI','Nimble-WEB']\" \"PostProcessing=['rescale']\" \"200<ProbeWidth<=3000\" => 3 queries\n",
    "\n",
    "-qp queryPartition\n",
    " * Uses one factor query to evaluate algorithm performance on a partitioned dataset through its individual sub-queries (e.g. \"Collection==['NC2017'] & Purpose==['add','remove']\" will average over \"Collection==['NC2017'] & Purpose==['add']\" and \"Collection==['NC2017'] & Purpose==['remove']\" for a total of two queried averages). This option generates a single report table (CSV) that contains M partition results, one result for each query.\n",
    "   * Syntax: -qp \"query\"\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI']\" => 1 partition\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI','Nimble-WEB'] & PostProcessing=['rescale']\" => 2 partitions\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI','Nimble-WEB'] & PostProcessing=['rescale','noise']\" => 4 partitions\n",
    "   \n",
    "-qm queryManipulation\n",
    " * Filters the dataset before scoring takes place for some number of queries. It is functionally similar to the -q query option. The option generates M report tables (CSV), one for each query.\n",
    "   * Syntax: -qm \"query1\" \"query2\" \"query3\" ...\n",
    "   - Like the -q option, the syntax is the same as Pandas' query syntax.\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -qm \"Purpose==['remove']\" => 1 query\n",
    "   \n",
    "   % -qm \"Operation==['PasteSplice']\" \"Operation==['FillContentAwareFill']\" => 2 query\n",
    "   \n",
    "   % -qm \"Purpose==['remove']\" \"Purpose==['add']\" \"Purpose==['splice']\"=> 3 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examples<a name=\"examples\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the tests, run the code below to generate the masks for the sample reference and system output. The masks for these test cases can take up at least 0.5 GB each otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd )\"\n",
    "TESTDIR=$(realpath $DIR/../../data/test_suite/videoSpatialLocalizationScorerTests)\n",
    "python2 $TESTDIR/gen_masks_for_ds.py -ds $TESTDIR\n",
    "python2 $TESTDIR/gen_spatial_mask.py -s $TESTDIR/p-vsltest_1/p-vsltest_1.csv -x $TESTDIR/indexes/MFC18_Dev2-manipulation-video-index.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs a test over these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd )\"\n",
    "TESTDIR=$(realpath $DIR/../../data/test_suite/videoSpatialLocalizationScorerTests)\n",
    "\n",
    "python2 $DIR/VideoSpatialLocalizationScorer.py -t manipulation\\\n",
    "                                          --refDir $TESTDIR\\\n",
    "                                          --sysDir $TESTDIR/p-vsltest_1\\\n",
    "                                          -r reference/manipulation-video/MFC18_Dev2-manipulation-video-ref.csv\\\n",
    "                                          -s p-vsltest_1.csv\\\n",
    "                                          -x indexes/MFC18_Dev2-manipulation-video-index.csv\\\n",
    "                                          -oR $DIR/test1/test1\\\n",
    "                                          --truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code will generate an aggregate report of the computed mask scores titled test1_mask_scores.csv and a per-image score report titled test1_pervideo.csv for the manipulation task, in the tools/VideoSpatialLocalizationScorer/test1 directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also select which manipulation regions to score based on other factors in the reference files. Other regions are dilated by a separate factor and counted as selective no-score zones in addition to the boundary no-score zones applied to the regions of interest. Multiple pre-filters can be applied independently to the data, resulting in the output of multiple output indices corresponding to the number of queries passed to -qm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates selective scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd )\"\n",
    "TESTDIR=$(realpath $DIR/../../data/test_suite/videoSpatialLocalizationScorerTests)\n",
    "\n",
    "python2 $DIR/VideoSpatialLocalizationScorer.py -t manipulation\\\n",
    "                                          --refDir $TESTDIR\\\n",
    "                                          --sysDir $TESTDIR/p-vsltest_1\\\n",
    "                                          -r reference/manipulation-video/MFC18_Dev2-manipulation-video-ref.csv\\\n",
    "                                          -s p-vsltest_1.csv\\\n",
    "                                          -x indexes/MFC18_Dev2-manipulation-video-index.csv\\\n",
    "                                          -oR $DIR/sel1/sel1\\\n",
    "                                          -qm \"Operation == 'PasteSampled'\"\\\n",
    "                                          --truncate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This software was developed at the National Institute of Standards\n",
    "and Technology (NIST) by employees of the Federal Government in the\n",
    "course of their official duties. Pursuant to Title 17 Section 105\n",
    "of the United States Code, this software is not subject to copyright\n",
    "protection and is in the public domain. NIST assumes no responsibility\n",
    "whatsoever for use by other parties of its source code or open source\n",
    "server, and makes no guarantees, expressed or implied, about its quality,\n",
    "reliability, or any other characteristic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py2]",
   "language": "python",
   "name": "conda-env-ipykernel_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
