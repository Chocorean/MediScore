{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "0\\. [Synopsis](#Synopsis)\n",
    "\n",
    "1\\. [Description](#Description)\n",
    "\n",
    "2\\. [Reading the Masks](#Readmasks)\n",
    "\n",
    "3\\. [Binarizing the Reference Mask](#Binmasks)\n",
    "\n",
    "4\\. [Generating No-Score Zones](#noscorezone)\n",
    "\n",
    "5\\. [Metrics](#metrics)\n",
    "\n",
    "5.1 [System Mask Thresholding](#metricsthresholding)\n",
    "\n",
    "5.2 [Notation](#metricsnotation)\n",
    "\n",
    "5.3 [Matthews Correlation Coefficient (MCC)](#metricsmcc)\n",
    "\n",
    "5.4 [Nimble Mask Metric (NMM)](#metricsnmm)\n",
    "\n",
    "5.5 [Weighted L1 Loss (BWL1 and GWL1)](#metricswl1)\n",
    "\n",
    "5.6 [Area Under Curve (AUC)](#metricsauc)\n",
    "\n",
    "5.7 [Equal Error Rate (EER)](#metricseer)\n",
    "\n",
    "6\\. [ROC Curves](#roccurves)\n",
    "\n",
    "7\\. [Options](#options)\n",
    "\n",
    "7.1 [Task Type Options](#optionstask)\n",
    "\n",
    "7.2 [Input Options](#optionsinputs)\n",
    "\n",
    "7.3 [Output Options](#optionsoutputs)\n",
    "\n",
    "7.4 [Scoring Options](#optionsscoring)\n",
    "\n",
    "7.5 [Performance Evaluation by Query](#optionsquery)\n",
    "\n",
    "7.6 [Report Options](#optionsreport)\n",
    "\n",
    "8\\. [Examples](#examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Synopsis<a name=\"Synopsis\"></a>\n",
    "\n",
    "python2 MaskScorer.py -t task-type<br/>\n",
    "                      --refDir reference-directory<br/>\n",
    "                      -r reference-file<br/>\n",
    "                      -x index-file<br/>\n",
    "                      -s system-output-file<br/>\n",
    "                      --outRoot output-directory<br/>\n",
    "                      [options]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description<a name=\"Description\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mask scorer calculates performance scores that measure the accuracy of the system output masks to their corresponding reference masks. The script produces output files in the form of pipe-separated (\"|\") CSV files, one containing scores at the trial level (\\*-mask_scores-perimage.csv), another containing an average of the metrics in the first CSV (\\*-mask_score.csv), and a third containing a list of manipulations that were evaluated or skipped over (\\*-journalResults.csv). If the -html option is selected, the script will generate a detailed HTML file for the mask region performance results.\n",
    "\n",
    "The mask scorer takes the following steps to score a mask:\n",
    "\n",
    "1. Reads in the reference and system masks.\n",
    "2. Binarizes the reference mask.\n",
    "3. Generates the no-score zone.\n",
    "4. Computes metrics \n",
    "\n",
    "If the script aborts due to an error, the error will be written to standard out, and the script will exit with a status of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading the Masks<a name=\"Readmasks\"></a>\n",
    "\n",
    "The system mask is read in as a single-layer grayscale png. (Note that the validator will fail the mask if it is not.)\n",
    "\n",
    "The reference mask is read in as either a single-layer JPEG2000 containing bit values corresponding to localizable manipulations or a color png if the evaluation is against color reference masks where mutually exclusive regions are RGB color-coded and different regions are easily distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binarizing the Reference Mask<a name=\"Binmasks\"></a>\n",
    "\n",
    "In order to score the system output mask against the reference mask, the reference mask also needs to be converted to a single-channel mask. The default way to do this would be to treat each of the regions with bit values (color regions for the color masks) as manipulated regions and to score accordingly.\n",
    "\n",
    "If selective scoring is utilized (via the -qm option), only regions that match the query will be scored against. The rest will be treated as no-score regions, to be discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generating No-Score Zones<a name=\"noscorezone\"></a>\n",
    "\n",
    "No-score zones are automatically established for three reasons: to grant an amount of flexibility to the performer, to exclude manipulated regions that are not of interest, and to exclude regions that the performer does not deem of interest. Each subset of the no-score zone is denoted by a specified type of no-score zone based on these motivations: the boundary no-score zone, selective no-score zone, and pixel no-score zone respectively.\n",
    "\n",
    "To ease the demand in identifying the region manipulated, the boundary around each region is eroded and dilated, with the difference serving as the boundary no-score zone.\n",
    "\n",
    "If selective scoring is utilized (via the -qm option), the mask scorer also generates the distraction no-score zone by dilating the positive pixels of the irrelevant regions in the reference mask. This number need not be the same as the dilation parameter used to generate the boundary no-score zone and may be adjusted via option --ntdks.\n",
    "\n",
    "An additional system-generated no-score zone may be used by the performer. The performer specifies a particular pixel value to treat as a no-score zone via option --nspx, and all pixels with the same value in the system mask serves as the no-score zone.\n",
    "\n",
    "The three are merged to generate the overall no-score zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics<a name=\"metrics\"></a>\n",
    "\n",
    "The system computes the following metrics:\n",
    " * Matthews Correlation Coefficient (MCC)\n",
    " * Nimble Mask Metric (NMM)\n",
    " * Weighted L1 Loss: Binary (BWL1) and Grayscale (GWL1)\n",
    " * Area Under the ROC Curve (AUC)\n",
    " * Equal Error Rate (EER)\n",
    "\n",
    "### 5.1 System Mask Thresholding<a name=\"metricsthresholding\"></a>\n",
    "\n",
    "Most of these metrics depend on the binarized values of the grayscale system mask. There are various ways binarize the system mask, namely by picking the threshold that determines whether to treat a particular grayscale pixel as white or black.\n",
    "\n",
    "One is to threshold each mask according to whether a threshold returns the greatest MCC; these are called the \"optimum\" metrics. Another is to specify a fixed threshold for all masks to be binarized with the --sbin option; the metrics computed from this fixed threshold are called the \"actual\" metrics. Specifying said threshold will also prompt the scorer to compute the threshold that yields the maximum average MCC; the metrics computed by this threshold are called the \"maximum\" metrics.\n",
    "\n",
    "Averages of the metrics will be recorded across all probes. The average and standard deviation of the optimum thresholds will also be recorded.\n",
    "\n",
    "### 5.2 Notation<a name=\"metricsnotation\"></a>\n",
    "\n",
    "The following notation is used in discussing the metrics:\n",
    " * $gt$ refers to the binarized reference mask discussed in section 3.\n",
    " * $sys_{\\theta}$ refers to the system output mask binarized by specified threshold $\\theta$\n",
    "   * $sys_{*}$ refers to the unbinarized grayscale system output mask\n",
    " * $wts$ is a matrix of the overall no-score region discussed in section 4.\n",
    "   * The no-score region can be further subdivided into the boundary, distraction, and system-generated no-score zones. The pixel counts corresponding to these regions are denoted $BNS$, $SNS$, and $PNS$ respectively. In cases where regions from different types of no-score zone intersect, the pixels count towards $PNS$, then $SNS$, or $BNS$, in order of decreasing priority.\n",
    " * $TP$, $TN$, $FP$, and $FN$ are functions of $gt$, $sys_{\\theta}$, and $wts$. All four are measures derived from the confusion matrix. Pixels for which the weights matrix is equal to 0 are not counted in the confusion measures:\n",
    "   * $TP$ refers to the total number of True Positives, pixels where $gt$ is positive and $sys_{\\theta}$ is thresholded positive (manipulated)\n",
    "   * $TN$ refers to the total number of True Negatives, pixels where $gt$ is negative and $sys_{\\theta}$ is positive (not manipulated)\n",
    "   * $FP$ refers to the total number of False Positives, pixels where $gt$ is negative but $sys_{\\theta}$ is positive.\n",
    "   * $FN$ refers to the total number of False Negatives, pixels where $gt$ is positive but $sys_{\\theta}$ is negative.\n",
    "\n",
    "### 5.3 Matthews Correlation Coefficient (MCC)<a name=\"metricsmcc\"></a>\n",
    "\\begin{equation*}\n",
    "MCC(gt,sys_{\\theta}) = \\frac{TP*TN - FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n",
    "\\end{equation*}\n",
    "\n",
    "An MCC of 1 denotes perfect correlation, an MCC of 0 denotes no correlation at all, and an MCC of -1 denotes perfect anti-correlation. If any of the sums in the denominator equals 0, the MCC is taken to be 0 by convention.\n",
    "\n",
    "### 5.4 Nimble Mask Metric (NMM)<a name=\"metricsnmm\"></a>\n",
    "\\begin{equation*}\n",
    "NMM(gt,sys_{\\theta},wts,c)=\\max{\\left(\\frac{TP - FN - FP}{\\sum\\limits_{px\\in gt}wts(px)},c\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\Sigma_{px \\in GT}$ refers to the sum over the pixels in the ground truth that are marked black. $c$ denotes a minimum cutoff value for the scoring to have any meaning; by default, $c=-1$.\n",
    "\n",
    "### 5.5 Weighted L1 Loss (BWL1 and GWL1)<a name=\"metricswl1\"></a>\n",
    "\n",
    "A Weighted L1 of 0 denotes perfect or near perfect match up to variation within the weights that are 0; 1 denotes perfect mismatch. $(FP+FN)_{weights > 0}$ refers to the total number of $FP$ and $FN$ pixels where weights are greater than 0.\n",
    "\n",
    "The Weighted L1 is applied separately to the original grayscale system output and the binarized mask, producing the grayscale Weighted L1 (GWL1) and binarized Weighted L1 (BWL1) metrics respectively. In the case of the original grayscale, the value is summed over the weighted difference in pixel intensity.\n",
    "\n",
    "\\begin{equation*}\n",
    "BWL1(gt,sys_{\\theta},wts)=\\frac{(FP+FN)_{wts > 0}}{\\sum\\limits_{px\\in gt}wts(px)}\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "GWL1(gt,sys_{*},wts)=\\frac{\\sum\\limits_{px\\in gt} \\left|gt(px) - sys_{*}(px)\\right|wts(px)}{\\sum\\limits_{px\\in gt}wts(px)}\n",
    "\\end{equation*}\n",
    "\n",
    "### 5.6 Area Under Curve (AUC)<a name=\"metricsauc\"></a>\n",
    "\n",
    "The possibility of multiple thresholds giving rise to a set of varying confusion measures gives rise to the receiver operating characteristic (ROC). The ROC curve measures a relation between the true positive rate ($TPR$) and false positive rate ($FPR$), defined as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "FPR = \\frac{FP}{FP + TN}\n",
    "\\end{equation*}\n",
    "\n",
    "The area under the ROC curve (AUC) is a measure for the accuracy of the system.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "AUC(gt,sys_{*},wts) = \\sum\\limits_{\\theta_{n} \\in \\{\\theta_{0},\\ldots,\\theta_{N-1}\\}} \\left(TPR(gt,sys_{\\theta_{n+1}},wts) + TPR(gt,sys_{\\theta_{n}},wts)\\right)\\left(FPR(gt,sys_{\\theta_{n+1}},wts) - FPR(gt,sys_{\\theta_{n}},wts)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "### 5.7 Equal Error Rate (EER)<a name=\"metricseer\"></a>\n",
    "The value of the $FPR$ at the threshold where it is equal to the $FNR$ (or vice versa). $FPR$ is given by the formula in 5.6. $FNR$ is simply $1 - TPR$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ROC Curves<a name=\"roccurves\"></a>\n",
    "\n",
    "A pixel-weighted average ROC curve and probe-weighted average ROC curve will be recorded and defined across all thresholds relevant to all system masks. For a given threshold $\\theta$, the true positive rate (TPR) for the ROC curves are as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR_{pixel}(\\theta) = \\frac{\\sum_{probes}TP_{\\theta}}{\\sum_{probes}\\left(TP_{\\theta} + FN_{\\theta}\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "TPR_{probe}(\\theta) = \\frac{1}{|probes|}\\sum_{probes}\\frac{TP_{\\theta}}{TP_{\\theta} + FN_{\\theta}}\n",
    "\\end{equation*}\n",
    "\n",
    "The false postive rates (FPR) are defined similarly. The pixel weighted average and probe weighted average ROC curves are drawn from the pairs of (FPR,TPR) points. Their AUC's are recorded with the average metrics mentioned at the end of section 5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Options<a name=\"options\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command-line options for the mask scorer can be categorized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Task Type Options<a name=\"optionstask\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-t --task [manipulation, splice]\n",
    "\n",
    "  * Specify the task type for evaluation (default = manipulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Input Options<a name=\"optionsinputs\"></a>\n",
    "\n",
    "All CSV files passed to the Mask Scorer must contain headers and must have their rows separated by pipe characters ('|'). Fields and values in the CSV should <i>not</i> be enclosed in quotes ( ' or \" ) if possible (e.g. entries 'foo', an empty field, and 'bar', in that order, should look like this in the csv: foo||bar). Additional specifications for the index and system output files can be found in the ValidatorNotebook.html file under the Validator directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--refDir\n",
    "\n",
    "  * Specify the reference and index data path (e.g. \"/NC2016_Test0601\") (default = .)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-r --inRef\n",
    "\n",
    "  * Specify the reference CSV file within refDir that contains the ground-truth information and metadata about each image. Key fields are TaskID, ProbeFileID, ProbeFileName, and ProbeMaskFileName, and if scoring on the 'splice' task, DonorFileID, DonorFileName, and DonorMaskFileName as well. Often the File ID's for the Probe and Donor will be the same as the file names, minus the extension. Additional fields, especially metadata pertaining to the ground-truth manipulation, may be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-x --inIndex\n",
    "\n",
    "  * Define the index CSV file within refDir. The index file contains the TaskID, ProbeFileID, ProbeFileName, ProbeWidth, and ProbeHeight fields, and if scoring on the splice task, the DonorFileID, DonorFileName, DonorWidth, and DonorHeight fields as well. No additional fields are permitted for the index file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--sysDir\n",
    "\n",
    "  * Specify the system output data path, for example \"mysysoutput/\" (default = .) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-s --inSys\n",
    "\n",
    "  * Specify the CSV file of the system performance results formatted according to NC2016 specification. The file must contain the ProbeFileID, ConfidenceScore, and ProbeOutputMaskFileName fields, in that order, and if scoring on the splice task, the ProbeFileID, DonorFileID, ConfidenceScore, ProbeOutputMaskFileName, and DonorOutputMaskFileName fields, in that order. The ProbeOutputMaskFileNames and DonorOutputMaskFileNames (where relevant) should be directory strings relative to the location of the system performance CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--sbin\n",
    "\n",
    "  * Evaluate the system output masks as binarized masks with a numeric threshold in the interval [0,255]. Choosing -1 will forego this option. (default = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--color\n",
    "\n",
    "  * Evaluate the system output against colorized referenced masks. Individual regions in the colorized masks are identifiable by region and do not intersect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Output Options<a name=\"optionsoutputs\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--outRoot\n",
    "\n",
    "  * Specify the report output path and the file prefix joined by a '/'. For example, specifying \"--outRoot test/output\" for a submission NIST_001 will output the aggregate score report \"output_mask_score.csv\" and the per-image report \"output_mask_scores_perimage.csv\" in the \"test\" directory.\n",
    "\n",
    "--outMeta\n",
    "  * Save the CSV file with the system output with minimal metadata. This is a separate file than the normal outputs.\n",
    "\n",
    "--outAllMeta\n",
    "  * Save the CSV file with the system output with all available metadata. This is a separate file than the normal outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Scoring Options<a name=\"optionsscoring\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--eks\n",
    "\n",
    "  * Erosion kernel size. (number must be odd; default = 15)\n",
    "  \n",
    "--dks\n",
    "\n",
    "  * Dilation kernel size. (number must be odd; default = 11)\n",
    "\n",
    "--ntdks\n",
    "\n",
    "  * Non-target dilation kernel for regions that the user does not want scored. (number must be odd; default = 15)\n",
    "\n",
    "--nspx\n",
    "\n",
    "  * Set a pixel value in the system output mask to be a no-score region. The pixel value must be in the interval [0,255]. -1 indicates that no particular pixel value will be chosen to be the no-score zone. (default = -1)\n",
    "\n",
    "-k kernel\n",
    "\n",
    "  * The shape of the kernel to be used, for both erosion and dilation. Choose from 'box','disc','diamond','gaussian', or 'line'. The default kernel is 'box'.\n",
    "  \n",
    "-xF indexFilter\n",
    "\n",
    "  * Filter scoring to only files that are present in the index file. This option permits scoring to select index files for the purpose of testing, and may accept system outputs that have not passed the validator.\n",
    "\n",
    "--speedup\n",
    "\n",
    "  * Run mask evaluation with a sped-up evaluator.\n",
    "  \n",
    "-p processors\n",
    "\n",
    "  * Run mask evaluation with multiple processors to speed up computation further. (default = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Performance Evaluation by Query<a name=\"optionsquery\"></a>\n",
    "\n",
    "This option allows the user to evaluate their algorithm performance on either subsets or partitions of the data based on the specified queries and query options. The reference and index CSV files contain a list of factors (e.g., ProbePostProcessed|DonorPostProcessed|ManipulationQuality|IsManipulationTypeRemoval|...). Selecting none of the following factors will output a single report table (CSV) over the entire computed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-q query\n",
    " * Evaluate algorithm performance on a partitioned dataset using multiple factor queries, one at a time (e.g. \"Collection==['NC2017'] & Purpose==['add','remove']\" will average over the rows that fit this criterion for one queried average, but \"Collection==['NC2017']\" \"Purpose==['add','remove']\" will average over the first and then the second independently for two queried averages). The option generates N report tables (CSV), one for each query.\n",
    "   * Syntax: -q \"query1\" \"query2\" \"query3\" ...\n",
    "   - The syntax is the same as Pandas' query syntax. Please see the detailed query rule in the website: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-query.\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI']\" => 1 query\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI'] and PostProcessing=['rescale']\" => 1 query\n",
    "   \n",
    "   % -q \"Collection==['Nimble-SCI','Nimble-WEB']\" \"PostProcessing=['rescale']\" \"200<ProbeWidth<=3000\" => 3 queries\n",
    "\n",
    "-qp queryPartition\n",
    " * Uses one factor query to evaluate algorithm performance on a partitioned dataset through its individual sub-queries (e.g. \"Collection==['NC2017'] & Purpose==['add','remove']\" will average over \"Collection==['NC2017'] & Purpose==['add']\" and \"Collection==['NC2017'] & Purpose==['remove']\" for a total of two queried averages). This option generates a single report table (CSV) that contains M partition results, one result for each query.\n",
    "   * Syntax: -qp \"query\"\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI']\" => 1 partition\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI','Nimble-WEB'] & PostProcessing=['rescale']\" => 2 partitions\n",
    "   \n",
    "   % -qp \"Collection==['Nimble-SCI','Nimble-WEB'] & PostProcessing=['rescale','noise']\" => 4 partitions\n",
    "   \n",
    "-qm queryManipulation\n",
    " * Filters the dataset before scoring takes place for some number of queries. It is functionally similar to the -q query option. The option generates M report tables (CSV), one for each query.\n",
    "   * Syntax: -qm \"query1\" \"query2\" \"query3\" ...\n",
    "   - Like the -q option, the syntax is the same as Pandas' query syntax.\n",
    "   \n",
    "   Examples:\n",
    "   \n",
    "   % -qm \"Purpose==['remove']\" => 1 query\n",
    "   \n",
    "   % -qm \"Operation==['PasteSplice']\" \"Operation==['FillContentAwareFill']\" => 2 query\n",
    "   \n",
    "   % -qm \"Purpose==['remove']\" \"Purpose==['add']\" \"Purpose==['splice']\"=> 3 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Report Options<a name=\"optionsreport\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-v verbose\n",
    "\n",
    "  * Control print output. Select 1 to print all non-error related output and 0 to suppress all print output (bar argument-parsing errors).\n",
    "  \n",
    "--precision\n",
    "\n",
    "  * The number of digits to round computed scores, (e.g. a score of 0.3333333333333... will round to 0.33333 for a precision of 5), (default = 16).\n",
    "\n",
    "-html\n",
    "\n",
    "  * Output the report to HTML files to visualize scoring.\n",
    "  \n",
    "--optOut\n",
    "\n",
    "  * Evaluate algorithm performance on a select number of trials determined by the performer via values in the ProbeStatus column. The values in the column that are opted out of the evaluation are \"NonProcessed\" (denoting some kind of system failure on that probe), \"OptOutLocalization\" (denoting opting out of the localization task only), and \"OptOutAll\" (denoting opting out of both the detection and localization tasks),.\n",
    "  \n",
    "--displayScoredOnly\n",
    "\n",
    "  * Display only the data for which a localized score could be generated. This will exclude images for which there are no score-able regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Examples<a name=\"examples\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current query: ConfidenceScore < 0.5\n",
      "Current query: ManMade==['no']\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python MaskScorer.py -t manipulation --refDir ../../data/test_suite/maskScorerTests \\\n",
    "-r reference/manipulation/NC2017-manipulation-ref.csv -x indexes/NC2017-manipulation-index.csv \\\n",
    "-s ../../data/test_suite/maskScorerTests/B_NC2017_Manipulation_ImgOnly_c-me2_1/B_NC2017_Manipulation_ImgOnly_c-me2_1.csv \\\n",
    "-oR outputs/maniptest -html -q \"ConfidenceScore < 0.5\" \"ManMade==['no']\" --color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code in the tools/MaskScorer directory will generate an aggregate report of the computed mask scores titled B_NC2017_Manipulation_ImgOnly_c-me2_1-mask_scores.csv and a per-image score report titled B_NC2017_Manipulation_ImgOnly_c-me2_1-mask_scores_perimage.csv for the manipulation task.\n",
    "\n",
    "The -html flag is also set, allowing the code to generate an HTML per-image <a href=\"outputs/maniptest/index.html\">index file</a> with the scores and metadata containing links to individual detailed reports of each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user may also select which manipulation regions to score, depending on the manipulations listed under the \"Purpose\" column in the journalMask file. Other regions are dilated by a separate factor and counted as selective no-score zones in addition to the boundary no-score zones applied to the regions of interest. Multiple pre-filters can be applied independently to the data, resulting in the output of multiple output indices corresponding to the number of queries passed to -qm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python MaskScorer.py -t manipulation --refDir ../../data/test_suite/maskScorerTests \\\n",
    "-r reference/manipulation/NC2017-manipulation-ref.csv -x indexes/NC2017-manipulation-index.csv \\\n",
    "-s ../../data/test_suite/maskScorerTests/B_NC2017_Manipulation_ImgOnly_c-me2_1/B_NC2017_Manipulation_ImgOnly_c-me2_1.csv \\\n",
    "-oR outputs/maniptargets -html -qm \"Purpose==['remove']\" \"Purpose==['clone','add']\" --color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample HTML index files for the 'remove' and 'clone','add' operations can be found <a href=\"outputs/maniptargets/index_0/index.html\">here</a> and <a href=\"outputs/maniptargets/index_1/index.html\">here</a> respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "This software was developed at the National Institute of Standards\n",
    "and Technology (NIST) by employees of the Federal Government in the\n",
    "course of their official duties. Pursuant to Title 17 Section 105\n",
    "of the United States Code, this software is not subject to copyright\n",
    "protection and is in the public domain. NIST assumes no responsibility\n",
    "whatsoever for use by other parties of its source code or open source\n",
    "server, and makes no guarantees, expressed or implied, about its quality,\n",
    "reliability, or any other characteristic."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ipykernel_py2]",
   "language": "python",
   "name": "conda-env-ipykernel_py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
